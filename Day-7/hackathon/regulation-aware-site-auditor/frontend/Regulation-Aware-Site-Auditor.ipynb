{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPS9a2ocTYNX",
        "outputId": "4c49a7f8-6e71-4925-ff57-6ac4a396822f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement contrast-ratio (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for contrast-ratio\u001b[0m\u001b[31m\n",
            "\u001b[0m‚úÖ All dependencies installed successfully!\n",
            "üîë Remember to add your Gemini API key in the GEMINI_API_KEY variable above\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "!pip install -q gradio\n",
        "!pip install -q requests\n",
        "!pip install -q beautifulsoup4\n",
        "!pip install -q google-generativeai\n",
        "!pip install -q langchain\n",
        "!pip install -q chromadb\n",
        "!pip install -q selenium\n",
        "!pip install -q webdriver-manager\n",
        "!pip install -q pillow\n",
        "!pip install -q python-dotenv\n",
        "!pip install -q validators\n",
        "!pip install -q contrast-ratio\n",
        "\n",
        "# Import all necessary libraries\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin, urlparse\n",
        "import gradio as gr\n",
        "import google.generativeai as genai\n",
        "from PIL import Image, ImageColor\n",
        "import validators\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "GEMINI_API_KEY = \"\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")\n",
        "print(\"üîë Remember to add your Gemini API key in the GEMINI_API_KEY variable above\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0txe1YG2bmnQ",
        "outputId": "0982f702-1dca-4f7e-c48e-3acecc6c4c05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Website Scraper initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# AI-Powered Site Auditor - Section 2: Website Scraper and Content Analyzer\n",
        "\n",
        "class WebsiteScraper:\n",
        "    def __init__(self):\n",
        "        self.headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        }\n",
        "\n",
        "    def fetch_website_content(self, url):\n",
        "        \"\"\"Fetch and parse website content\"\"\"\n",
        "        try:\n",
        "            if not validators.url(url):\n",
        "                return None, \"Invalid URL format\"\n",
        "\n",
        "            response = requests.get(url, headers=self.headers, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Extract key elements\n",
        "            content_data = {\n",
        "                'url': url,\n",
        "                'title': soup.find('title').get_text() if soup.find('title') else 'No title',\n",
        "                'html_content': str(soup),\n",
        "                'meta_tags': self.extract_meta_tags(soup),\n",
        "                'images': self.extract_images(soup, url),\n",
        "                'forms': self.extract_forms(soup),\n",
        "                'links': self.extract_links(soup, url),\n",
        "                'scripts': self.extract_scripts(soup),\n",
        "                'css_styles': self.extract_css_styles(soup),\n",
        "                'headings': self.extract_headings(soup),\n",
        "                'content_text': soup.get_text()[:5000]  # First 5000 chars\n",
        "            }\n",
        "\n",
        "            return content_data, None\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            return None, f\"Error fetching website: {str(e)}\"\n",
        "        except Exception as e:\n",
        "            return None, f\"Error parsing website: {str(e)}\"\n",
        "\n",
        "    def extract_meta_tags(self, soup):\n",
        "        \"\"\"Extract meta tags information\"\"\"\n",
        "        meta_tags = {}\n",
        "        for meta in soup.find_all('meta'):\n",
        "            name = meta.get('name') or meta.get('property') or meta.get('http-equiv')\n",
        "            content = meta.get('content')\n",
        "            if name and content:\n",
        "                meta_tags[name] = content\n",
        "        return meta_tags\n",
        "\n",
        "    def extract_images(self, soup, base_url):\n",
        "        \"\"\"Extract image information\"\"\"\n",
        "        images = []\n",
        "        for img in soup.find_all('img'):\n",
        "            img_data = {\n",
        "                'src': urljoin(base_url, img.get('src', '')),\n",
        "                'alt': img.get('alt', ''),\n",
        "                'title': img.get('title', ''),\n",
        "                'width': img.get('width', ''),\n",
        "                'height': img.get('height', '')\n",
        "            }\n",
        "            images.append(img_data)\n",
        "        return images\n",
        "\n",
        "    def extract_forms(self, soup):\n",
        "        \"\"\"Extract form information\"\"\"\n",
        "        forms = []\n",
        "        for form in soup.find_all('form'):\n",
        "            form_data = {\n",
        "                'action': form.get('action', ''),\n",
        "                'method': form.get('method', 'GET'),\n",
        "                'inputs': []\n",
        "            }\n",
        "            for input_tag in form.find_all(['input', 'textarea', 'select']):\n",
        "                input_data = {\n",
        "                    'type': input_tag.get('type', 'text'),\n",
        "                    'name': input_tag.get('name', ''),\n",
        "                    'required': input_tag.has_attr('required'),\n",
        "                    'label': self.find_label_for_input(soup, input_tag)\n",
        "                }\n",
        "                form_data['inputs'].append(input_data)\n",
        "            forms.append(form_data)\n",
        "        return forms\n",
        "\n",
        "    def find_label_for_input(self, soup, input_tag):\n",
        "        \"\"\"Find associated label for input field\"\"\"\n",
        "        input_id = input_tag.get('id')\n",
        "        if input_id:\n",
        "            label = soup.find('label', {'for': input_id})\n",
        "            if label:\n",
        "                return label.get_text().strip()\n",
        "        return ''\n",
        "\n",
        "    def extract_links(self, soup, base_url):\n",
        "        \"\"\"Extract link information\"\"\"\n",
        "        links = []\n",
        "        for link in soup.find_all('a', href=True):\n",
        "            link_data = {\n",
        "                'href': urljoin(base_url, link['href']),\n",
        "                'text': link.get_text().strip(),\n",
        "                'title': link.get('title', ''),\n",
        "                'target': link.get('target', '')\n",
        "            }\n",
        "            links.append(link_data)\n",
        "        return links\n",
        "\n",
        "    def extract_scripts(self, soup):\n",
        "        \"\"\"Extract script information\"\"\"\n",
        "        scripts = []\n",
        "        for script in soup.find_all('script'):\n",
        "            script_data = {\n",
        "                'src': script.get('src', ''),\n",
        "                'type': script.get('type', ''),\n",
        "                'content': script.string[:200] if script.string else ''  # First 200 chars\n",
        "            }\n",
        "            scripts.append(script_data)\n",
        "        return scripts\n",
        "\n",
        "    def extract_css_styles(self, soup):\n",
        "        \"\"\"Extract CSS style information\"\"\"\n",
        "        styles = []\n",
        "        for style in soup.find_all('style'):\n",
        "            if style.string:\n",
        "                styles.append(style.string[:500])  # First 500 chars\n",
        "        return styles\n",
        "\n",
        "    def extract_headings(self, soup):\n",
        "        \"\"\"Extract heading structure\"\"\"\n",
        "        headings = []\n",
        "        for level in range(1, 7):\n",
        "            for heading in soup.find_all(f'h{level}'):\n",
        "                headings.append({\n",
        "                    'level': level,\n",
        "                    'text': heading.get_text().strip(),\n",
        "                    'id': heading.get('id', '')\n",
        "                })\n",
        "        return headings\n",
        "\n",
        "# Initialize the scraper\n",
        "scraper = WebsiteScraper()\n",
        "\n",
        "print(\"‚úÖ Website Scraper initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKRy4AuDbnfQ",
        "outputId": "eed52832-bf26-4e9b-951c-a8560c6e1f64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Compliance Checker initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# AI-Powered Site Auditor - Section 3: Compliance Checker and Rules Engine\n",
        "\n",
        "class ComplianceChecker:\n",
        "    def __init__(self):\n",
        "        self.compliance_rules = {\n",
        "            'gdpr': self.check_gdpr_compliance,\n",
        "            'accessibility': self.check_accessibility_compliance,\n",
        "            'wcag': self.check_wcag_compliance,\n",
        "            'seo': self.check_seo_compliance,\n",
        "            'security': self.check_security_compliance\n",
        "        }\n",
        "\n",
        "    def analyze_compliance(self, website_data):\n",
        "        \"\"\"Run all compliance checks\"\"\"\n",
        "        results = {\n",
        "            'url': website_data['url'],\n",
        "            'title': website_data['title'],\n",
        "            'issues': [],\n",
        "            'recommendations': [],\n",
        "            'score': 0,\n",
        "            'categories': {}\n",
        "        }\n",
        "\n",
        "        for category, check_function in self.compliance_rules.items():\n",
        "            category_results = check_function(website_data)\n",
        "            results['categories'][category] = category_results\n",
        "            results['issues'].extend(category_results['issues'])\n",
        "            results['recommendations'].extend(category_results['recommendations'])\n",
        "\n",
        "        # Calculate overall score\n",
        "        total_checks = sum(len(cat['issues']) + len(cat['passed']) for cat in results['categories'].values())\n",
        "        passed_checks = sum(len(cat['passed']) for cat in results['categories'].values())\n",
        "        results['score'] = round((passed_checks / total_checks * 100) if total_checks > 0 else 0, 1)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def check_gdpr_compliance(self, data):\n",
        "        \"\"\"Check GDPR compliance requirements\"\"\"\n",
        "        issues = []\n",
        "        recommendations = []\n",
        "        passed = []\n",
        "\n",
        "        html_content = data['html_content'].lower()\n",
        "\n",
        "        # Check for cookie consent\n",
        "        cookie_keywords = ['cookie', 'consent', 'gdpr', 'privacy policy', 'data protection']\n",
        "        has_cookie_consent = any(keyword in html_content for keyword in cookie_keywords)\n",
        "\n",
        "        if not has_cookie_consent:\n",
        "            issues.append({\n",
        "                'severity': 'High',\n",
        "                'category': 'GDPR',\n",
        "                'issue': 'Missing cookie consent mechanism',\n",
        "                'description': 'No cookie consent banner or privacy controls detected'\n",
        "            })\n",
        "            recommendations.append({\n",
        "                'category': 'GDPR',\n",
        "                'recommendation': 'Add a cookie consent banner that allows users to accept/reject cookies',\n",
        "                'priority': 'High'\n",
        "            })\n",
        "        else:\n",
        "            passed.append('Cookie consent mechanism present')\n",
        "\n",
        "        # Check for privacy policy\n",
        "        privacy_links = [link for link in data['links'] if 'privacy' in link['text'].lower() or 'privacy' in link['href'].lower()]\n",
        "        if not privacy_links:\n",
        "            issues.append({\n",
        "                'severity': 'High',\n",
        "                'category': 'GDPR',\n",
        "                'issue': 'Missing privacy policy link',\n",
        "                'description': 'No privacy policy link found on the website'\n",
        "            })\n",
        "            recommendations.append({\n",
        "                'category': 'GDPR',\n",
        "                'recommendation': 'Add a clearly visible link to your privacy policy',\n",
        "                'priority': 'High'\n",
        "            })\n",
        "        else:\n",
        "            passed.append('Privacy policy link present')\n",
        "\n",
        "        # Check for data collection forms without consent\n",
        "        for form in data['forms']:\n",
        "            has_email_input = any(inp['type'] == 'email' for inp in form['inputs'])\n",
        "            has_consent_checkbox = any('consent' in inp['name'].lower() or 'privacy' in inp['name'].lower() for inp in form['inputs'])\n",
        "\n",
        "            if has_email_input and not has_consent_checkbox:\n",
        "                issues.append({\n",
        "                    'severity': 'Medium',\n",
        "                    'category': 'GDPR',\n",
        "                    'issue': 'Form collecting email without explicit consent',\n",
        "                    'description': 'Email collection form lacks consent checkbox'\n",
        "                })\n",
        "                recommendations.append({\n",
        "                    'category': 'GDPR',\n",
        "                    'recommendation': 'Add a consent checkbox to forms collecting personal data',\n",
        "                    'priority': 'Medium'\n",
        "                })\n",
        "\n",
        "        return {'issues': issues, 'recommendations': recommendations, 'passed': passed}\n",
        "\n",
        "    def check_accessibility_compliance(self, data):\n",
        "        \"\"\"Check accessibility compliance (ADA/WCAG)\"\"\"\n",
        "        issues = []\n",
        "        recommendations = []\n",
        "        passed = []\n",
        "\n",
        "        # Check for missing alt text\n",
        "        images_without_alt = [img for img in data['images'] if not img['alt']]\n",
        "        if images_without_alt:\n",
        "            issues.append({\n",
        "                'severity': 'High',\n",
        "                'category': 'Accessibility',\n",
        "                'issue': f'{len(images_without_alt)} images missing alt text',\n",
        "                'description': 'Images without alt text are not accessible to screen readers'\n",
        "            })\n",
        "            recommendations.append({\n",
        "                'category': 'Accessibility',\n",
        "                'recommendation': 'Add descriptive alt text to all images',\n",
        "                'priority': 'High'\n",
        "            })\n",
        "        else:\n",
        "            passed.append('All images have alt text')\n",
        "\n",
        "        # Check for proper heading structure\n",
        "        headings = data['headings']\n",
        "        if headings:\n",
        "            # Check if starts with h1\n",
        "            first_heading = min(headings, key=lambda x: x['level'])\n",
        "            if first_heading['level'] != 1:\n",
        "                issues.append({\n",
        "                    'severity': 'Medium',\n",
        "                    'category': 'Accessibility',\n",
        "                    'issue': 'Page does not start with H1',\n",
        "                    'description': 'Proper heading hierarchy should start with H1'\n",
        "                })\n",
        "                recommendations.append({\n",
        "                    'category': 'Accessibility',\n",
        "                    'recommendation': 'Ensure page has a proper H1 heading as the main title',\n",
        "                    'priority': 'Medium'\n",
        "                })\n",
        "            else:\n",
        "                passed.append('Proper heading structure with H1')\n",
        "\n",
        "        # Check for form labels\n",
        "        for form in data['forms']:\n",
        "            inputs_without_labels = [inp for inp in form['inputs'] if not inp['label'] and inp['type'] not in ['submit', 'button', 'hidden']]\n",
        "            if inputs_without_labels:\n",
        "                issues.append({\n",
        "                    'severity': 'High',\n",
        "                    'category': 'Accessibility',\n",
        "                    'issue': f'Form inputs without labels detected',\n",
        "                    'description': 'Form inputs need associated labels for screen readers'\n",
        "                })\n",
        "                recommendations.append({\n",
        "                    'category': 'Accessibility',\n",
        "                    'recommendation': 'Add proper labels to all form inputs',\n",
        "                    'priority': 'High'\n",
        "                })\n",
        "\n",
        "        # Check for skip navigation\n",
        "        soup = BeautifulSoup(data['html_content'], 'html.parser')\n",
        "        skip_nav = soup.find('a', string=re.compile(r'skip.*nav', re.I))\n",
        "        if not skip_nav:\n",
        "            issues.append({\n",
        "                'severity': 'Medium',\n",
        "                'category': 'Accessibility',\n",
        "                'issue': 'Missing skip navigation link',\n",
        "                'description': 'Skip navigation helps keyboard users bypass repetitive content'\n",
        "            })\n",
        "            recommendations.append({\n",
        "                'category': 'Accessibility',\n",
        "                'recommendation': 'Add a \"Skip to main content\" link at the beginning of the page',\n",
        "                'priority': 'Medium'\n",
        "            })\n",
        "        else:\n",
        "            passed.append('Skip navigation link present')\n",
        "\n",
        "        return {'issues': issues, 'recommendations': recommendations, 'passed': passed}\n",
        "\n",
        "    def check_wcag_compliance(self, data):\n",
        "        \"\"\"Check WCAG specific guidelines\"\"\"\n",
        "        issues = []\n",
        "        recommendations = []\n",
        "        passed = []\n",
        "\n",
        "        # Check for language attribute\n",
        "        soup = BeautifulSoup(data['html_content'], 'html.parser')\n",
        "        html_tag = soup.find('html')\n",
        "        if not html_tag or not html_tag.get('lang'):\n",
        "            issues.append({\n",
        "                'severity': 'Medium',\n",
        "                'category': 'WCAG',\n",
        "                'issue': 'Missing language attribute',\n",
        "                'description': 'HTML tag should have a lang attribute for screen readers'\n",
        "            })\n",
        "            recommendations.append({\n",
        "                'category': 'WCAG',\n",
        "                'recommendation': 'Add lang attribute to HTML tag (e.g., <html lang=\"en\">)',\n",
        "                'priority': 'Medium'\n",
        "            })\n",
        "        else:\n",
        "            passed.append('Language attribute present')\n",
        "\n",
        "        # Check for page title\n",
        "        if not data['title'] or data['title'] == 'No title':\n",
        "            issues.append({\n",
        "                'severity': 'High',\n",
        "                'category': 'WCAG',\n",
        "                'issue': 'Missing page title',\n",
        "                'description': 'Page title is essential for screen readers and SEO'\n",
        "            })\n",
        "            recommendations.append({\n",
        "                'category': 'WCAG',\n",
        "                'recommendation': 'Add a descriptive title tag to the page',\n",
        "                'priority': 'High'\n",
        "            })\n",
        "        else:\n",
        "            passed.append('Page title present')\n",
        "\n",
        "        # Check for focus indicators (basic check)\n",
        "        css_content = ' '.join(data['css_styles']).lower()\n",
        "        has_focus_styles = ':focus' in css_content\n",
        "        if not has_focus_styles:\n",
        "            issues.append({\n",
        "                'severity': 'Medium',\n",
        "                'category': 'WCAG',\n",
        "                'issue': 'No custom focus indicators detected',\n",
        "                'description': 'Custom focus styles improve keyboard navigation visibility'\n",
        "            })\n",
        "            recommendations.append({\n",
        "                'category': 'WCAG',\n",
        "                'recommendation': 'Add visible focus indicators for interactive elements',\n",
        "                'priority': 'Medium'\n",
        "            })\n",
        "        else:\n",
        "            passed.append('Focus indicators present')\n",
        "\n",
        "        return {'issues': issues, 'recommendations': recommendations, 'passed': passed}\n",
        "\n",
        "    def check_seo_compliance(self, data):\n",
        "        \"\"\"Check basic SEO compliance\"\"\"\n",
        "        issues = []\n",
        "        recommendations = []\n",
        "        passed = []\n",
        "\n",
        "        # Check meta description\n",
        "        meta_description = data['meta_tags'].get('description', '')\n",
        "        if not meta_description:\n",
        "            issues.append({\n",
        "                'severity': 'Medium',\n",
        "                'category': 'SEO',\n",
        "                'issue': 'Missing meta description',\n",
        "                'description': 'Meta description helps search engines understand page content'\n",
        "            })\n",
        "            recommendations.append({\n",
        "                'category': 'SEO',\n",
        "                'recommendation': 'Add a compelling meta description (150-160 characters)',\n",
        "                'priority': 'Medium'\n",
        "            })\n",
        "        else:\n",
        "            passed.append('Meta description present')\n",
        "\n",
        "        # Check for multiple H1 tags\n",
        "        h1_count = len([h for h in data['headings'] if h['level'] == 1])\n",
        "        if h1_count > 1:\n",
        "            issues.append({\n",
        "                'severity': 'Low',\n",
        "                'category': 'SEO',\n",
        "                'issue': f'Multiple H1 tags found ({h1_count})',\n",
        "                'description': 'Multiple H1 tags can confuse search engines'\n",
        "            })\n",
        "            recommendations.append({\n",
        "                'category': 'SEO',\n",
        "                'recommendation': 'Use only one H1 tag per page',\n",
        "                'priority': 'Low'\n",
        "            })\n",
        "        elif h1_count == 1:\n",
        "            passed.append('Single H1 tag present')\n",
        "\n",
        "        return {'issues': issues, 'recommendations': recommendations, 'passed': passed}\n",
        "\n",
        "    def check_security_compliance(self, data):\n",
        "        \"\"\"Check basic security compliance\"\"\"\n",
        "        issues = []\n",
        "        recommendations = []\n",
        "        passed = []\n",
        "\n",
        "        # Check if site is HTTPS\n",
        "        if not data['url'].startswith('https://'):\n",
        "            issues.append({\n",
        "                'severity': 'High',\n",
        "                'category': 'Security',\n",
        "                'issue': 'Site not using HTTPS',\n",
        "                'description': 'HTTPS is essential for secure data transmission'\n",
        "            })\n",
        "            recommendations.append({\n",
        "                'category': 'Security',\n",
        "                'recommendation': 'Implement SSL certificate and redirect HTTP to HTTPS',\n",
        "                'priority': 'High'\n",
        "            })\n",
        "        else:\n",
        "            passed.append('HTTPS encryption enabled')\n",
        "\n",
        "        # Check for mixed content\n",
        "        soup = BeautifulSoup(data['html_content'], 'html.parser')\n",
        "        http_resources = []\n",
        "        for tag in soup.find_all(['img', 'script', 'link']):\n",
        "            src = tag.get('src') or tag.get('href', '')\n",
        "            if src.startswith('http://'):\n",
        "                http_resources.append(src)\n",
        "\n",
        "        if http_resources and data['url'].startswith('https://'):\n",
        "            issues.append({\n",
        "                'severity': 'Medium',\n",
        "                'category': 'Security',\n",
        "                'issue': f'Mixed content detected ({len(http_resources)} resources)',\n",
        "                'description': 'HTTP resources on HTTPS pages can cause security warnings'\n",
        "            })\n",
        "            recommendations.append({\n",
        "                'category': 'Security',\n",
        "                'recommendation': 'Update all resource URLs to use HTTPS',\n",
        "                'priority': 'Medium'\n",
        "            })\n",
        "\n",
        "        return {'issues': issues, 'recommendations': recommendations, 'passed': passed}\n",
        "\n",
        "# Initialize the compliance checker\n",
        "compliance_checker = ComplianceChecker()\n",
        "\n",
        "print(\"‚úÖ Compliance Checker initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYUVQ54VbquP",
        "outputId": "741a02ba-e881-435e-e5c8-34ba8cc5be9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ AI Analyzer class ready!\n",
            "üí° AI analyzer will be initialized when you provide your Gemini API key\n"
          ]
        }
      ],
      "source": [
        "# AI-Powered Site Auditor - Section 4: AI-Powered Analysis with Gemini\n",
        "\n",
        "class AIAnalyzer:\n",
        "    def __init__(self, api_key):\n",
        "        if not api_key:\n",
        "            raise ValueError(\"Gemini API key is required\")\n",
        "\n",
        "        genai.configure(api_key=api_key)\n",
        "        self.model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "\n",
        "    def analyze_website_with_ai(self, website_data, compliance_results):\n",
        "        \"\"\"Use Gemini to provide intelligent analysis and recommendations\"\"\"\n",
        "        try:\n",
        "            # Prepare context for AI analysis\n",
        "            context = self.prepare_analysis_context(website_data, compliance_results)\n",
        "\n",
        "            # Generate AI analysis\n",
        "            ai_insights = self.generate_ai_insights(context)\n",
        "\n",
        "            # Generate priority recommendations\n",
        "            priority_recommendations = self.generate_priority_recommendations(context)\n",
        "\n",
        "            # Generate improvement suggestions\n",
        "            improvement_suggestions = self.generate_improvement_suggestions(context)\n",
        "\n",
        "            # Generate analysis summary\n",
        "            analysis_summary = self.generate_analysis_summary(compliance_results)\n",
        "\n",
        "\n",
        "            return {\n",
        "                'ai_insights': ai_insights,\n",
        "                'priority_recommendations': priority_recommendations,\n",
        "                'improvement_suggestions': improvement_suggestions,\n",
        "                'analysis_summary': analysis_summary\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {\n",
        "                'ai_insights': f\"AI analysis failed: {str(e)}\",\n",
        "                'priority_recommendations': [],\n",
        "                'improvement_suggestions': [],\n",
        "                'analysis_summary': \"AI analysis unavailable\"\n",
        "            }\n",
        "\n",
        "    def prepare_analysis_context(self, website_data, compliance_results):\n",
        "        \"\"\"Prepare context for AI analysis\"\"\"\n",
        "        context = {\n",
        "            'url': website_data['url'],\n",
        "            'title': website_data['title'],\n",
        "            'total_issues': len(compliance_results['issues']),\n",
        "            'compliance_score': compliance_results['score'],\n",
        "            'issues_by_category': {},\n",
        "            'website_structure': {\n",
        "                'images_count': len(website_data['images']),\n",
        "                'forms_count': len(website_data['forms']),\n",
        "                'links_count': len(website_data['links']),\n",
        "                'headings_count': len(website_data['headings'])\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Group issues by category and severity\n",
        "        for issue in compliance_results['issues']:\n",
        "            category = issue['category']\n",
        "            if category not in context['issues_by_category']:\n",
        "                context['issues_by_category'][category] = {'High': 0, 'Medium': 0, 'Low': 0}\n",
        "            context['issues_by_category'][category][issue['severity']] += 1\n",
        "\n",
        "        return context\n",
        "\n",
        "    def generate_ai_insights(self, context):\n",
        "        \"\"\"Generate AI-powered insights about the website\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Analyze this website compliance audit data and provide intelligent insights:\n",
        "\n",
        "        Website: {context['url']}\n",
        "        Title: {context['title']}\n",
        "        Compliance Score: {context['compliance_score']}%\n",
        "        Total Issues: {context['total_issues']}\n",
        "\n",
        "        Issues by Category:\n",
        "        {json.dumps(context['issues_by_category'], indent=2)}\n",
        "\n",
        "        Website Structure:\n",
        "        - Images: {context['website_structure']['images_count']}\n",
        "        - Forms: {context['website_structure']['forms_count']}\n",
        "        - Links: {context['website_structure']['links_count']}\n",
        "        - Headings: {context['website_structure']['headings_count']}\n",
        "\n",
        "        Provide 3-4 key insights about the website's compliance status, focusing on:\n",
        "        1. Overall compliance health\n",
        "        2. Most critical areas needing attention\n",
        "        3. Positive aspects that are working well\n",
        "        4. Business impact of current issues\n",
        "\n",
        "        Keep insights concise and actionable.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Failed to generate AI insights: {str(e)}\"\n",
        "\n",
        "    def generate_priority_recommendations(self, context):\n",
        "        \"\"\"Generate prioritized recommendations\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Based on this website compliance audit, provide the top 5 priority recommendations:\n",
        "\n",
        "        Website: {context['url']}\n",
        "        Compliance Score: {context['compliance_score']}%\n",
        "        Total Issues: {context['total_issues']}\n",
        "\n",
        "        Issues by Category:\n",
        "        {json.dumps(context['issues_by_category'], indent=2)}\n",
        "\n",
        "        Provide exactly 5 recommendations in order of priority, considering:\n",
        "        1. Legal compliance risk\n",
        "        2. User experience impact\n",
        "        3. Implementation difficulty\n",
        "        4. Business impact\n",
        "\n",
        "        Format each recommendation as:\n",
        "        - Priority X: [Brief title] - [Concise description and why it's important]\n",
        "\n",
        "        Focus on actionable items that can be implemented quickly.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Failed to generate priority recommendations: {str(e)}\"\n",
        "\n",
        "    def generate_improvement_suggestions(self, context):\n",
        "        \"\"\"Generate specific improvement suggestions\"\"\"\n",
        "        prompt = f\"\"\"\n",
        "        Provide specific, technical improvement suggestions for this website:\n",
        "\n",
        "        Website: {context['url']}\n",
        "        Compliance Score: {context['compliance_score']}%\n",
        "\n",
        "        Issues by Category:\n",
        "        {json.dumps(context['issues_by_category'], indent=2)}\n",
        "\n",
        "        Provide concrete, implementable suggestions with code examples where appropriate.\n",
        "        Focus on:\n",
        "        1. GDPR compliance improvements\n",
        "        2. Accessibility enhancements\n",
        "        3. Technical implementation tips\n",
        "        4. Tools and resources that can help\n",
        "\n",
        "        Keep suggestions practical and include specific HTML/CSS examples when relevant.\n",
        "        \"\"\"\n",
        "\n",
        "        try:\n",
        "            response = self.model.generate_content(prompt)\n",
        "            return response.text\n",
        "        except Exception as e:\n",
        "            return f\"Failed to generate improvement suggestions: {str(e)}\"\n",
        "\n",
        "    def generate_analysis_summary(self, compliance_results):\n",
        "        \"\"\"Generate a summary of the analysis\"\"\"\n",
        "        total_issues = len(compliance_results['issues'])\n",
        "        high_severity = len([i for i in compliance_results['issues'] if i['severity'] == 'High'])\n",
        "        medium_severity = len([i for i in compliance_results['issues'] if i['severity'] == 'Medium'])\n",
        "        low_severity = len([i for i in compliance_results['issues'] if i['severity'] == 'Low'])\n",
        "\n",
        "        summary = f\"\"\"\n",
        "        üìä **Analysis Summary**\n",
        "\n",
        "        **Overall Score:** {compliance_results['score']}%\n",
        "        **Total Issues Found:** {total_issues}\n",
        "\n",
        "        **Issue Breakdown:**\n",
        "        - üî¥ High Priority: {high_severity} issues\n",
        "        - üü° Medium Priority: {medium_severity} issues\n",
        "        - üü¢ Low Priority: {low_severity} issues\n",
        "\n",
        "        **Categories Analyzed:**\n",
        "        - GDPR Compliance\n",
        "        - Accessibility (ADA/WCAG)\n",
        "        - SEO Optimization\n",
        "        - Security Best Practices\n",
        "        \"\"\"\n",
        "\n",
        "        return summary\n",
        "\n",
        "# Function to initialize AI analyzer (will be called when API key is provided)\n",
        "def initialize_ai_analyzer(api_key):\n",
        "    \"\"\"Initialize the AI analyzer with API key\"\"\"\n",
        "    if not api_key:\n",
        "        return None\n",
        "    try:\n",
        "        return AIAnalyzer(api_key)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to initialize AI analyzer: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ AI Analyzer class ready!\")\n",
        "print(\"üí° AI analyzer will be initialized when you provide your Gemini API key\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcicozv4wu0A",
        "outputId": "e1abbd32-b311-4e4b-d1ec-777c7fc4dbd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è RAG Module initialization failed: \u001b[91mYou are using a deprecated configuration of Chroma.\n",
            "\n",
            "\u001b[94mIf you do not have data you wish to migrate, you only need to change how you construct\n",
            "your Chroma client. Please see the \"New Clients\" section of https://docs.trychroma.com/deployment/migration.\n",
            "________________________________________________________________________________________________\n",
            "\n",
            "If you do have data you wish to migrate, we have a migration tool you can use in order to\n",
            "migrate your data to the new Chroma architecture.\n",
            "Please `pip install chroma-migrate` and run `chroma-migrate` to migrate your data and then\n",
            "change how you construct your Chroma client.\n",
            "\n",
            "See https://docs.trychroma.com/deployment/migration for more information or join our discord at https://discord.gg/MMeYNTmh3x for help!\u001b[0m\n",
            "üí° RAG features will be limited without proper initialization\n"
          ]
        }
      ],
      "source": [
        "# AI-Powered Site Auditor - Section 5: RAG Module for Live Regulatory Content\n",
        "\n",
        "import chromadb\n",
        "from chromadb.config import Settings\n",
        "\n",
        "class RAGModule:\n",
        "    def __init__(self):\n",
        "        self.client = chromadb.Client(Settings(\n",
        "            chroma_db_impl=\"duckdb+parquet\",\n",
        "            persist_directory=\"./chroma_db\"\n",
        "        ))\n",
        "        self.collection_name = \"compliance_regulations\"\n",
        "        self.regulatory_sources = {\n",
        "            'gdpr': 'https://gdpr.eu/what-is-gdpr/',\n",
        "            'wcag': 'https://www.w3.org/WAI/WCAG21/quickref/',\n",
        "            'ada': 'https://www.ada.gov/resources/web-guidance/'\n",
        "        }\n",
        "        self.initialize_collection()\n",
        "\n",
        "    def initialize_collection(self):\n",
        "        \"\"\"Initialize or get the regulatory content collection\"\"\"\n",
        "        try:\n",
        "            self.collection = self.client.get_collection(self.collection_name)\n",
        "            print(\"üìö Existing RAG collection loaded\")\n",
        "        except:\n",
        "            self.collection = self.client.create_collection(\n",
        "                name=self.collection_name,\n",
        "                metadata={\"description\": \"Compliance regulations and guidelines\"}\n",
        "            )\n",
        "            self.populate_initial_content()\n",
        "            print(\"üìö New RAG collection created and populated\")\n",
        "\n",
        "    def populate_initial_content(self):\n",
        "        \"\"\"Populate collection with initial regulatory content\"\"\"\n",
        "        initial_content = [\n",
        "            {\n",
        "                'id': 'gdpr_cookies',\n",
        "                'content': 'GDPR Article 7 requires clear and specific consent for cookie usage. Websites must provide users with the ability to withdraw consent as easily as giving it. Cookie consent banners should clearly explain what cookies are used for and allow granular control over cookie categories.',\n",
        "                'source': 'GDPR',\n",
        "                'category': 'cookies',\n",
        "                'metadata': {'regulation': 'GDPR', 'article': '7', 'topic': 'consent'}\n",
        "            },\n",
        "            {\n",
        "                'id': 'gdpr_privacy_policy',\n",
        "                'content': 'Under GDPR Article 13, organizations must provide clear information about data processing through a privacy policy. This must include the purpose of processing, legal basis, data retention periods, and contact details of the data protection officer.',\n",
        "                'source': 'GDPR',\n",
        "                'category': 'privacy',\n",
        "                'metadata': {'regulation': 'GDPR', 'article': '13', 'topic': 'transparency'}\n",
        "            },\n",
        "            {\n",
        "                'id': 'wcag_alt_text',\n",
        "                'content': 'WCAG 2.1 Success Criterion 1.1.1 requires that all non-text content has text alternatives that serve the equivalent purpose. Images must have alt attributes that describe the content and function of the image for screen reader users.',\n",
        "                'source': 'WCAG',\n",
        "                'category': 'accessibility',\n",
        "                'metadata': {'standard': 'WCAG 2.1', 'criterion': '1.1.1', 'level': 'A'}\n",
        "            },\n",
        "            {\n",
        "                'id': 'wcag_keyboard_navigation',\n",
        "                'content': 'WCAG 2.1 Success Criterion 2.1.1 states that all functionality must be available from a keyboard. Interactive elements should be reachable and operable using only keyboard navigation, with visible focus indicators.',\n",
        "                'source': 'WCAG',\n",
        "                'category': 'accessibility',\n",
        "                'metadata': {'standard': 'WCAG 2.1', 'criterion': '2.1.1', 'level': 'A'}\n",
        "            },\n",
        "            {\n",
        "                'id': 'wcag_color_contrast',\n",
        "                'content': 'WCAG 2.1 Success Criterion 1.4.3 requires a contrast ratio of at least 4.5:1 for normal text and 3:1 for large text against their background. This ensures text is readable for users with visual impairments.',\n",
        "                'source': 'WCAG',\n",
        "                'category': 'accessibility',\n",
        "                'metadata': {'standard': 'WCAG 2.1', 'criterion': '1.4.3', 'level': 'AA'}\n",
        "            },\n",
        "            {\n",
        "                'id': 'wcag_page_titles',\n",
        "                'content': 'WCAG 2.1 Success Criterion 2.4.2 requires that web pages have titles that describe topic or purpose. Page titles should be descriptive and help users understand what page they are on.',\n",
        "                'source': 'WCAG',\n",
        "                'category': 'accessibility',\n",
        "                'metadata': {'standard': 'WCAG 2.1', 'criterion': '2.4.2', 'level': 'A'}\n",
        "            },\n",
        "            {\n",
        "                'id': 'ada_web_accessibility',\n",
        "                'content': 'The Americans with Disabilities Act (ADA) applies to websites as places of public accommodation. Courts have increasingly held that websites must be accessible to people with disabilities, often referencing WCAG 2.1 Level AA as the standard.',\n",
        "                'source': 'ADA',\n",
        "                'category': 'legal',\n",
        "                'metadata': {'law': 'ADA', 'standard_reference': 'WCAG 2.1 AA'}\n",
        "            },\n",
        "            {\n",
        "                'id': 'gdpr_data_processing',\n",
        "                'content': 'GDPR Article 6 establishes lawful bases for processing personal data. Consent must be freely given, specific, informed and unambiguous. Pre-ticked boxes or silence do not constitute valid consent.',\n",
        "                'source': 'GDPR',\n",
        "                'category': 'data_processing',\n",
        "                'metadata': {'regulation': 'GDPR', 'article': '6', 'topic': 'lawful_basis'}\n",
        "            },\n",
        "            {\n",
        "                'id': 'wcag_form_labels',\n",
        "                'content': 'WCAG 2.1 Success Criterion 3.3.2 requires that labels or instructions are provided when content requires user input. Form inputs must have associated labels that clearly describe the required information.',\n",
        "                'source': 'WCAG',\n",
        "                'category': 'accessibility',\n",
        "                'metadata': {'standard': 'WCAG 2.1', 'criterion': '3.3.2', 'level': 'A'}\n",
        "            },\n",
        "            {\n",
        "                'id': 'gdpr_data_portability',\n",
        "                'content': 'GDPR Article 20 grants individuals the right to data portability. Organizations must provide personal data in a structured, commonly used, and machine-readable format when requested.',\n",
        "                'source': 'GDPR',\n",
        "                'category': 'rights',\n",
        "                'metadata': {'regulation': 'GDPR', 'article': '20', 'topic': 'data_portability'}\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Add content to collection\n",
        "        documents = [item['content'] for item in initial_content]\n",
        "        metadatas = [item['metadata'] for item in initial_content]\n",
        "        ids = [item['id'] for item in initial_content]\n",
        "\n",
        "        self.collection.add(\n",
        "            documents=documents,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids\n",
        "        )\n",
        "\n",
        "    def fetch_live_regulatory_content(self, source_type='gdpr'):\n",
        "        \"\"\"Fetch live content from regulatory websites\"\"\"\n",
        "        try:\n",
        "            url = self.regulatory_sources.get(source_type)\n",
        "            if not url:\n",
        "                return None\n",
        "\n",
        "            response = requests.get(url, timeout=10)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Extract relevant content based on source type\n",
        "            if source_type == 'gdpr':\n",
        "                return self.extract_gdpr_content(soup)\n",
        "            elif source_type == 'wcag':\n",
        "                return self.extract_wcag_content(soup)\n",
        "            elif source_type == 'ada':\n",
        "                return self.extract_ada_content(soup)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to fetch {source_type} content: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def extract_gdpr_content(self, soup):\n",
        "        \"\"\"Extract GDPR-specific content\"\"\"\n",
        "        content_sections = []\n",
        "\n",
        "        # Look for article sections or important paragraphs\n",
        "        for section in soup.find_all(['section', 'article', 'div'], class_=re.compile(r'content|article|section')):\n",
        "            text = section.get_text().strip()\n",
        "            if len(text) > 100 and any(keyword in text.lower() for keyword in ['consent', 'cookie', 'privacy', 'data']):\n",
        "                content_sections.append({\n",
        "                    'content': text[:500],  # Limit to 500 chars\n",
        "                    'source': 'GDPR_Live',\n",
        "                    'category': 'gdpr_update'\n",
        "                })\n",
        "\n",
        "        return content_sections[:5]  # Limit to 5 sections\n",
        "\n",
        "    def extract_wcag_content(self, soup):\n",
        "        \"\"\"Extract WCAG-specific content\"\"\"\n",
        "        content_sections = []\n",
        "\n",
        "        # Look for guideline sections\n",
        "        for section in soup.find_all(['section', 'div'], class_=re.compile(r'guideline|criterion|success')):\n",
        "            text = section.get_text().strip()\n",
        "            if len(text) > 100 and any(keyword in text.lower() for keyword in ['accessibility', 'screen reader', 'keyboard', 'contrast']):\n",
        "                content_sections.append({\n",
        "                    'content': text[:500],\n",
        "                    'source': 'WCAG_Live',\n",
        "                    'category': 'wcag_update'\n",
        "                })\n",
        "\n",
        "        return content_sections[:5]\n",
        "\n",
        "    def extract_ada_content(self, soup):\n",
        "        \"\"\"Extract ADA-specific content\"\"\"\n",
        "        content_sections = []\n",
        "\n",
        "        # Look for guidance sections\n",
        "        for section in soup.find_all(['section', 'div', 'p'], class_=re.compile(r'guidance|content')):\n",
        "            text = section.get_text().strip()\n",
        "            if len(text) > 100 and any(keyword in text.lower() for keyword in ['website', 'accessibility', 'disability', 'accommodation']):\n",
        "                content_sections.append({\n",
        "                    'content': text[:500],\n",
        "                    'source': 'ADA_Live',\n",
        "                    'category': 'ada_update'\n",
        "                })\n",
        "\n",
        "        return content_sections[:5]\n",
        "\n",
        "    def update_regulatory_content(self):\n",
        "        \"\"\"Update collection with latest regulatory content\"\"\"\n",
        "        try:\n",
        "            for source_type in self.regulatory_sources.keys():\n",
        "                print(f\"Fetching {source_type.upper()} content...\")\n",
        "                new_content = self.fetch_live_regulatory_content(source_type)\n",
        "\n",
        "                if new_content:\n",
        "                    for i, content_item in enumerate(new_content):\n",
        "                        doc_id = f\"{source_type}_live_{i}_{int(time.time())}\"\n",
        "\n",
        "                        self.collection.add(\n",
        "                            documents=[content_item['content']],\n",
        "                            metadatas=[{\n",
        "                                'source': content_item['source'],\n",
        "                                'category': content_item['category'],\n",
        "                                'updated': time.strftime('%Y-%m-%d')\n",
        "                            }],\n",
        "                            ids=[doc_id]\n",
        "                        )\n",
        "\n",
        "                    print(f\"‚úÖ Added {len(new_content)} {source_type.upper()} items\")\n",
        "                else:\n",
        "                    print(f\"‚ö†Ô∏è No new {source_type.upper()} content found\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error updating regulatory content: {str(e)}\")\n",
        "\n",
        "    def query_regulatory_guidance(self, query, n_results=3):\n",
        "        \"\"\"Query the RAG system for relevant regulatory guidance\"\"\"\n",
        "        try:\n",
        "            results = self.collection.query(\n",
        "                query_texts=[query],\n",
        "                n_results=n_results\n",
        "            )\n",
        "\n",
        "            if results['documents'] and results['documents'][0]:\n",
        "                guidance = []\n",
        "                for i, doc in enumerate(results['documents'][0]):\n",
        "                    metadata = results['metadatas'][0][i] if results['metadatas'] else {}\n",
        "                    guidance.append({\n",
        "                        'content': doc,\n",
        "                        'source': metadata.get('source', 'Unknown'),\n",
        "                        'category': metadata.get('category', 'General'),\n",
        "                        'relevance_score': 1 - results['distances'][0][i] if results['distances'] else 0\n",
        "                    })\n",
        "                return guidance\n",
        "            else:\n",
        "                return []\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error querying regulatory guidance: {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def get_contextual_recommendations(self, compliance_issues):\n",
        "        \"\"\"Get contextual recommendations based on specific compliance issues\"\"\"\n",
        "        recommendations = []\n",
        "\n",
        "        for issue in compliance_issues:\n",
        "            # Create query based on issue category and description\n",
        "            query = f\"{issue['category']} {issue['issue']} compliance requirements\"\n",
        "\n",
        "            guidance = self.query_regulatory_guidance(query, n_results=2)\n",
        "\n",
        "            if guidance:\n",
        "                recommendations.append({\n",
        "                    'issue': issue['issue'],\n",
        "                    'category': issue['category'],\n",
        "                    'regulatory_guidance': guidance,\n",
        "                    'implementation_priority': issue['severity']\n",
        "                })\n",
        "\n",
        "        return recommendations\n",
        "\n",
        "# Initialize RAG module\n",
        "try:\n",
        "    rag_module = RAGModule()\n",
        "    print(\"‚úÖ RAG Module initialized successfully!\")\n",
        "    print(\"üí° Use rag_module.update_regulatory_content() to fetch latest regulations\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è RAG Module initialization failed: {str(e)}\")\n",
        "    print(\"üí° RAG features will be limited without proper initialization\")\n",
        "    rag_module = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbqxmfE4ww4p",
        "outputId": "6a7b7775-ab83-4794-b2f4-8f3bf6615e40"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Report Generator initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# AI-Powered Site Auditor - Section 6: Report Generator and Formatting\n",
        "\n",
        "class ReportGenerator:\n",
        "    def __init__(self):\n",
        "        self.severity_colors = {\n",
        "            'High': 'üî¥',\n",
        "            'Medium': 'üü°',\n",
        "            'Low': 'üü¢'\n",
        "        }\n",
        "        self.category_icons = {\n",
        "            'GDPR': 'üõ°Ô∏è',\n",
        "            'Accessibility': '‚ôø',\n",
        "            'WCAG': 'üìã',\n",
        "            'SEO': 'üîç',\n",
        "            'Security': 'üîí'\n",
        "        }\n",
        "\n",
        "    def generate_comprehensive_report(self, website_data, compliance_results, ai_analysis=None, rag_recommendations=None):\n",
        "        \"\"\"Generate a comprehensive audit report\"\"\"\n",
        "        report = {\n",
        "            'executive_summary': self.generate_executive_summary(compliance_results),\n",
        "            'detailed_findings': self.generate_detailed_findings(compliance_results),\n",
        "            'priority_matrix': self.generate_priority_matrix(compliance_results),\n",
        "            'implementation_roadmap': self.generate_implementation_roadmap(compliance_results),\n",
        "            'technical_details': self.generate_technical_details(website_data, compliance_results)\n",
        "        }\n",
        "\n",
        "        if ai_analysis:\n",
        "            report['ai_insights'] = ai_analysis\n",
        "\n",
        "        if rag_recommendations:\n",
        "            report['regulatory_guidance'] = rag_recommendations\n",
        "\n",
        "        return report\n",
        "\n",
        "    def generate_executive_summary(self, compliance_results):\n",
        "        \"\"\"Generate executive summary\"\"\"\n",
        "        total_issues = len(compliance_results['issues'])\n",
        "        high_issues = len([i for i in compliance_results['issues'] if i['severity'] == 'High'])\n",
        "        score = compliance_results['score']\n",
        "\n",
        "        # Determine risk level\n",
        "        if score >= 80:\n",
        "            risk_level = \"‚úÖ Low Risk\"\n",
        "            risk_description = \"Website shows good compliance practices with minor issues to address.\"\n",
        "        elif score >= 60:\n",
        "            risk_level = \"‚ö†Ô∏è Medium Risk\"\n",
        "            risk_description = \"Website has several compliance gaps that should be addressed promptly.\"\n",
        "        else:\n",
        "            risk_level = \"üö® High Risk\"\n",
        "            risk_description = \"Website has significant compliance issues requiring immediate attention.\"\n",
        "\n",
        "        summary = f\"\"\"\n",
        "## üìä Executive Summary\n",
        "\n",
        "**Website:** {compliance_results['url']}\n",
        "**Audit Date:** {time.strftime('%Y-%m-%d %H:%M:%S')}\n",
        "**Overall Compliance Score:** {score}%\n",
        "\n",
        "### Risk Assessment\n",
        "**Risk Level:** {risk_level}\n",
        "{risk_description}\n",
        "\n",
        "### Quick Stats\n",
        "- **Total Issues Found:** {total_issues}\n",
        "- **High Priority Issues:** {high_issues}\n",
        "- **Categories Analyzed:** {len(compliance_results['categories'])}\n",
        "\n",
        "### Key Findings\n",
        "\"\"\"\n",
        "\n",
        "        # Add category breakdown\n",
        "        for category, results in compliance_results['categories'].items():\n",
        "            issues_count = len(results['issues'])\n",
        "            if issues_count > 0:\n",
        "                icon = self.category_icons.get(category.upper(), 'üìå')\n",
        "                summary += f\"- {icon} **{category.upper()}:** {issues_count} issues found\\n\"\n",
        "\n",
        "        return summary\n",
        "\n",
        "    def generate_detailed_findings(self, compliance_results):\n",
        "        \"\"\"Generate detailed findings section\"\"\"\n",
        "        findings = \"## üîç Detailed Findings\\n\\n\"\n",
        "\n",
        "        for category, results in compliance_results['categories'].items():\n",
        "            if results['issues']:\n",
        "                icon = self.category_icons.get(category.upper(), 'üìå')\n",
        "                findings += f\"### {icon} {category.upper()} Compliance\\n\\n\"\n",
        "\n",
        "                for issue in results['issues']:\n",
        "                    severity_icon = self.severity_colors[issue['severity']]\n",
        "                    findings += f\"**{severity_icon} {issue['severity']} Priority:** {issue['issue']}\\n\"\n",
        "                    findings += f\"*Description:* {issue['description']}\\n\\n\"\n",
        "\n",
        "                # Add passed items\n",
        "                if results['passed']:\n",
        "                    findings += \"**‚úÖ Passed Requirements:**\\n\"\n",
        "                    for passed in results['passed']:\n",
        "                        findings += f\"- {passed}\\n\"\n",
        "                    findings += \"\\n\"\n",
        "\n",
        "        return findings\n",
        "\n",
        "    def generate_priority_matrix(self, compliance_results):\n",
        "        \"\"\"Generate priority matrix for issues\"\"\"\n",
        "        matrix = \"## üìà Priority Matrix\\n\\n\"\n",
        "\n",
        "        # Group issues by severity\n",
        "        issues_by_severity = {'High': [], 'Medium': [], 'Low': []}\n",
        "\n",
        "        for issue in compliance_results['issues']:\n",
        "            issues_by_severity[issue['severity']].append(issue)\n",
        "\n",
        "        for severity in ['High', 'Medium', 'Low']:\n",
        "            if issues_by_severity[severity]:\n",
        "                icon = self.severity_colors[severity]\n",
        "                matrix += f\"### {icon} {severity} Priority Issues ({len(issues_by_severity[severity])})\\n\\n\"\n",
        "\n",
        "                for i, issue in enumerate(issues_by_severity[severity], 1):\n",
        "                    category_icon = self.category_icons.get(issue['category'], 'üìå')\n",
        "                    matrix += f\"{i}. {category_icon} **{issue['issue']}** ({issue['category']})\\n\"\n",
        "                    matrix += f\"   - {issue['description']}\\n\\n\"\n",
        "\n",
        "        return matrix\n",
        "\n",
        "    def generate_implementation_roadmap(self, compliance_results):\n",
        "        \"\"\"Generate implementation roadmap\"\"\"\n",
        "        roadmap = \"## üó∫Ô∏è Implementation Roadmap\\n\\n\"\n",
        "\n",
        "        # Create phases based on severity and complexity\n",
        "        phases = {\n",
        "            'Phase 1 (Immediate - 1-2 weeks)': [],\n",
        "            'Phase 2 (Short-term - 1-2 months)': [],\n",
        "            'Phase 3 (Long-term - 3-6 months)': []\n",
        "        }\n",
        "\n",
        "        # Categorize recommendations\n",
        "        for recommendation in compliance_results['recommendations']:\n",
        "            if recommendation.get('priority') == 'High':\n",
        "                phases['Phase 1 (Immediate - 1-2 weeks)'].append(recommendation)\n",
        "            elif recommendation.get('priority') == 'Medium':\n",
        "                phases['Phase 2 (Short-term - 1-2 months)'].append(recommendation)\n",
        "            else:\n",
        "                phases['Phase 3 (Long-term - 3-6 months)'].append(recommendation)\n",
        "\n",
        "        for phase, recommendations in phases.items():\n",
        "            if recommendations:\n",
        "                roadmap += f\"### {phase}\\n\\n\"\n",
        "                for i, rec in enumerate(recommendations, 1):\n",
        "                    category_icon = self.category_icons.get(rec['category'], 'üìå')\n",
        "                    roadmap += f\"{i}. {category_icon} {rec['recommendation']}\\n\"\n",
        "                roadmap += \"\\n\"\n",
        "\n",
        "        return roadmap\n",
        "\n",
        "    def generate_technical_details(self, website_data, compliance_results):\n",
        "        \"\"\"Generate technical details section\"\"\"\n",
        "        details = \"## üîß Technical Details\\n\\n\"\n",
        "\n",
        "        details += f\"### Website Structure Analysis\\n\"\n",
        "        details += f\"- **Total Images:** {len(website_data['images'])}\\n\"\n",
        "        details += f\"- **Total Forms:** {len(website_data['forms'])}\\n\"\n",
        "        details += f\"- **Total Links:** {len(website_data['links'])}\\n\"\n",
        "        details += f\"- **Heading Structure:** {len(website_data['headings'])} headings\\n\"\n",
        "        details += f\"- **Meta Tags:** {len(website_data['meta_tags'])} tags found\\n\\n\"\n",
        "\n",
        "        # Image analysis\n",
        "        if website_data['images']:\n",
        "            images_without_alt = [img for img in website_data['images'] if not img['alt']]\n",
        "            details += f\"### Image Analysis\\n\"\n",
        "            details += f\"- **Images without alt text:** {len(images_without_alt)}\\n\"\n",
        "            details += f\"- **Alt text coverage:** {((len(website_data['images']) - len(images_without_alt)) / len(website_data['images']) * 100):.1f}%\\n\\n\"\n",
        "\n",
        "        # Form analysis\n",
        "        if website_data['forms']:\n",
        "            details += f\"### Form Analysis\\n\"\n",
        "            for i, form in enumerate(website_data['forms'], 1):\n",
        "                details += f\"**Form {i}:**\\n\"\n",
        "                details += f\"- Method: {form['method']}\\n\"\n",
        "                details += f\"- Action: {form['action'] or 'Same page'}\\n\"\n",
        "                details += f\"- Input fields: {len(form['inputs'])}\\n\\n\"\n",
        "\n",
        "        return details\n",
        "\n",
        "    def format_html_report(self, report_data):\n",
        "        \"\"\"Format report as HTML for better presentation\"\"\"\n",
        "        html_template = \"\"\"\n",
        "        <div style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif; line-height: 1.6; color: #333; max-width: 1200px; margin: 0 auto; padding: 20px;\">\n",
        "            <div style=\"background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 10px; margin-bottom: 30px; text-align: center;\">\n",
        "                <h1 style=\"margin: 0; font-size: 2.5em;\">üîç Website Compliance Audit Report</h1>\n",
        "                <p style=\"margin: 10px 0 0 0; font-size: 1.2em; opacity: 0.9;\">Comprehensive Analysis & Recommendations</p>\n",
        "            </div>\n",
        "\n",
        "            <div style=\"display: grid; grid-template-columns: repeat(auto-fit, minmax(250px, 1fr)); gap: 20px; margin-bottom: 30px;\">\n",
        "                <div style=\"background: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 4px solid #28a745;\">\n",
        "                    <h3 style=\"margin: 0 0 10px 0; color: #28a745;\">Compliance Score</h3>\n",
        "                    <div style=\"font-size: 2em; font-weight: bold;\">{score}%</div>\n",
        "                </div>\n",
        "                <div style=\"background: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 4px solid #dc3545;\">\n",
        "                    <h3 style=\"margin: 0 0 10px 0; color: #dc3545;\">Total Issues</h3>\n",
        "                    <div style=\"font-size: 2em; font-weight: bold;\">{total_issues}</div>\n",
        "                </div>\n",
        "                <div style=\"background: #f8f9fa; padding: 20px; border-radius: 8px; border-left: 4px solid #fd7e14;\">\n",
        "                    <h3 style=\"margin: 0 0 10px 0; color: #fd7e14;\">High Priority</h3>\n",
        "                    <div style=\"font-size: 2em; font-weight: bold;\">{high_issues}</div>\n",
        "                </div>\n",
        "            </div>\n",
        "\n",
        "            <div style=\"background: white; padding: 30px; border-radius: 10px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); margin-bottom: 20px;\">\n",
        "                {content}\n",
        "            </div>\n",
        "        </div>\n",
        "        \"\"\"\n",
        "\n",
        "        # Convert markdown to basic HTML formatting\n",
        "        content = report_data.replace('\\n## ', '\\n<h2>').replace('\\n### ', '\\n<h3>')\n",
        "        content = content.replace('**', '<strong>').replace('**', '</strong>')\n",
        "        content = content.replace('\\n- ', '\\n<li>').replace('\\n\\n', '</p><p>')\n",
        "        content = f\"<p>{content}</p>\"\n",
        "\n",
        "        return html_template.format(\n",
        "            content=content,\n",
        "            score=85,  # This would be dynamic\n",
        "            total_issues=12,  # This would be dynamic\n",
        "            high_issues=3  # This would be dynamic\n",
        "        )\n",
        "\n",
        "    def export_report_formats(self, report_data, base_filename=\"audit_report\"):\n",
        "        \"\"\"Export report in multiple formats\"\"\"\n",
        "        formats = {}\n",
        "\n",
        "        # Generate markdown format\n",
        "        markdown_content = \"\"\n",
        "        for section_name, section_content in report_data.items():\n",
        "            if isinstance(section_content, str):\n",
        "                markdown_content += f\"\\n{section_content}\\n\"\n",
        "            elif isinstance(section_content, dict):\n",
        "                markdown_content += f\"\\n## {section_name.replace('_', ' ').title()}\\n\"\n",
        "                for key, value in section_content.items():\n",
        "                    if isinstance(value, str):\n",
        "                        markdown_content += f\"\\n### {key.replace('_', ' ').title()}\\n{value}\\n\"\n",
        "\n",
        "        formats['markdown'] = markdown_content\n",
        "        formats['html'] = self.format_html_report(markdown_content)\n",
        "\n",
        "        # Generate JSON format for API consumption\n",
        "        formats['json'] = json.dumps(report_data, indent=2)\n",
        "\n",
        "        return formats\n",
        "\n",
        "# Initialize report generator\n",
        "report_generator = ReportGenerator()\n",
        "\n",
        "print(\"‚úÖ Report Generator initialized successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZCZWedjwxVr",
        "outputId": "b7a50ffe-82e6-4f7d-cdcb-4166be942b5e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Site Auditor App initialized!\n",
            "üí° Ready to build the Gradio interface\n"
          ]
        }
      ],
      "source": [
        "# AI-Powered Site Auditor - Section 7: Gradio Interface and Main Application\n",
        "\n",
        "class SiteAuditorApp:\n",
        "    def __init__(self):\n",
        "        self.scraper = WebsiteScraper()\n",
        "        self.compliance_checker = ComplianceChecker()\n",
        "        self.report_generator = ReportGenerator()\n",
        "        self.ai_analyzer = None\n",
        "        self.rag_module = rag_module  # From previous section\n",
        "\n",
        "    def initialize_ai(self, api_key):\n",
        "        \"\"\"Initialize AI analyzer with provided API key\"\"\"\n",
        "        if api_key and api_key.strip():\n",
        "            try:\n",
        "                self.ai_analyzer = AIAnalyzer(api_key.strip())\n",
        "                return \"‚úÖ AI Analyzer initialized successfully!\"\n",
        "            except Exception as e:\n",
        "                return f\"‚ùå Failed to initialize AI: {str(e)}\"\n",
        "        return \"‚ö†Ô∏è No API key provided\"\n",
        "\n",
        "    def audit_website(self, url, api_key=\"\", enable_ai=True, enable_rag=True, progress=gr.Progress()):\n",
        "        \"\"\"Main audit function\"\"\"\n",
        "        try:\n",
        "            # Progress tracking\n",
        "            progress(0.1, desc=\"Validating URL...\")\n",
        "\n",
        "            if not url or not url.strip():\n",
        "                return \"‚ùå Please enter a valid URL\", \"\", \"\"\n",
        "\n",
        "            url = url.strip()\n",
        "            if not url.startswith(('http://', 'https://')):\n",
        "                url = 'https://' + url\n",
        "\n",
        "            # Initialize AI if requested and API key provided\n",
        "            ai_status = \"\"\n",
        "            if enable_ai and api_key:\n",
        "                ai_status = self.initialize_ai(api_key)\n",
        "\n",
        "            # Step 1: Scrape website\n",
        "            progress(0.2, desc=\"Scraping website content...\")\n",
        "            website_data, error = self.scraper.fetch_website_content(url)\n",
        "\n",
        "            if error:\n",
        "                return f\"‚ùå Error: {error}\", \"\", \"\"\n",
        "\n",
        "            # Step 2: Run compliance checks\n",
        "            progress(0.4, desc=\"Analyzing compliance...\")\n",
        "            compliance_results = self.compliance_checker.analyze_compliance(website_data)\n",
        "\n",
        "            # Step 3: Generate AI analysis (if enabled)\n",
        "            ai_analysis = None\n",
        "            if enable_ai and self.ai_analyzer:\n",
        "                progress(0.6, desc=\"Generating AI insights...\")\n",
        "                try:\n",
        "                    ai_analysis = self.ai_analyzer.analyze_website_with_ai(website_data, compliance_results)\n",
        "                except Exception as e:\n",
        "                    ai_status += f\"\\n‚ö†Ô∏è AI analysis failed: {str(e)}\"\n",
        "\n",
        "            # Step 4: Get RAG recommendations (if enabled)\n",
        "            rag_recommendations = None\n",
        "            if enable_rag and self.rag_module:\n",
        "                progress(0.8, desc=\"Fetching regulatory guidance...\")\n",
        "                try:\n",
        "                    rag_recommendations = self.rag_module.get_contextual_recommendations(compliance_results['issues'])\n",
        "                except Exception as e:\n",
        "                    print(f\"RAG recommendations failed: {str(e)}\")\n",
        "\n",
        "            # Step 5: Generate comprehensive report\n",
        "            progress(0.9, desc=\"Generating report...\")\n",
        "            report_data = self.report_generator.generate_comprehensive_report(\n",
        "                website_data, compliance_results, ai_analysis, rag_recommendations\n",
        "            )\n",
        "\n",
        "            # Format outputs\n",
        "            progress(1.0, desc=\"Finalizing report...\")\n",
        "\n",
        "            # Main report (Markdown)\n",
        "            main_report = f\"{report_data['executive_summary']}\\n{report_data['detailed_findings']}\\n{report_data['priority_matrix']}\"\n",
        "\n",
        "            # AI Insights (if available)\n",
        "            ai_insights_text = \"\"\n",
        "            if ai_analysis:\n",
        "                ai_insights_text = f\"\"\"\n",
        "## ü§ñ AI-Powered Insights\n",
        "\n",
        "### Key Insights\n",
        "{ai_analysis.get('ai_insights', 'No insights available')}\n",
        "\n",
        "### Priority Recommendations\n",
        "{ai_analysis.get('priority_recommendations', 'No recommendations available')}\n",
        "\n",
        "### Improvement Suggestions\n",
        "{ai_analysis.get('improvement_suggestions', 'No suggestions available')}\n",
        "\"\"\"\n",
        "\n",
        "            # Implementation roadmap\n",
        "            implementation_roadmap = report_data['implementation_roadmap']\n",
        "\n",
        "            # Add AI status to main report if relevant\n",
        "            if ai_status:\n",
        "                main_report = f\"{ai_status}\\n\\n{main_report}\"\n",
        "\n",
        "            return main_report, ai_insights_text, implementation_roadmap\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Audit failed: {str(e)}\", \"\", \"\"\n",
        "\n",
        "    def update_rag_content(self, progress=gr.Progress()):\n",
        "        \"\"\"Update RAG content with latest regulations\"\"\"\n",
        "        if not self.rag_module:\n",
        "            return \"‚ùå RAG module not available\"\n",
        "\n",
        "        try:\n",
        "            progress(0.2, desc=\"Fetching GDPR content...\")\n",
        "            progress(0.5, desc=\"Fetching WCAG guidelines...\")\n",
        "            progress(0.8, desc=\"Fetching ADA guidance...\")\n",
        "            self.rag_module.update_regulatory_content()\n",
        "            return \"‚úÖ Regulatory content updated successfully!\"\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Failed to update regulatory content: {str(e)}\"\n",
        "\n",
        "# Initialize the app\n",
        "app = SiteAuditorApp()\n",
        "\n",
        "print(\"‚úÖ Site Auditor App initialized!\")\n",
        "print(\"üí° Ready to build the Gradio interface\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54865e70",
        "outputId": "05e30ed8-d25d-42ea-e691-ca8540b26a38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Gradio interface defined!\n",
            "üí° Ready to launch the app\n"
          ]
        }
      ],
      "source": [
        "# AI-Powered Site Auditor - Section 8: Gradio Interface Definition\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# AI-Powered Website Compliance Auditor\")\n",
        "    gr.Markdown(\"Enter a website URL to audit its compliance with GDPR, Accessibility (WCAG/ADA), SEO, and Security standards.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        url_input = gr.Textbox(label=\"Website URL\", placeholder=\"e.g., https://www.example.com\")\n",
        "        audit_button = gr.Button(\"Run Audit\")\n",
        "\n",
        "    with gr.Accordion(\"‚öôÔ∏è Advanced Settings\", open=False):\n",
        "        api_key_input = gr.Textbox(label=\"Gemini API Key (Optional, for AI insights)\", type=\"password\")\n",
        "        enable_ai_checkbox = gr.Checkbox(label=\"Enable AI-Powered Insights\", value=True)\n",
        "        enable_rag_checkbox = gr.Checkbox(label=\"Enable Regulatory Guidance (RAG)\", value=True)\n",
        "        update_rag_button = gr.Button(\"Update Regulatory Content (Takes time)\")\n",
        "        rag_update_status = gr.Textbox(label=\"RAG Update Status\", interactive=False)\n",
        "\n",
        "    with gr.Tabs():\n",
        "        with gr.TabItem(\"üìä Audit Report\"):\n",
        "            report_output = gr.Markdown(\"Report will appear here...\")\n",
        "        with gr.TabItem(\"ü§ñ AI Insights\"):\n",
        "            ai_insights_output = gr.Markdown(\"AI Insights will appear here...\")\n",
        "        with gr.TabItem(\"üó∫Ô∏è Implementation Roadmap\"):\n",
        "            roadmap_output = gr.Markdown(\"Implementation Roadmap will appear here...\")\n",
        "\n",
        "    audit_button.click(\n",
        "        app.audit_website,\n",
        "        inputs=[url_input, api_key_input, enable_ai_checkbox, enable_rag_checkbox],\n",
        "        outputs=[report_output, ai_insights_output, roadmap_output]\n",
        "    )\n",
        "\n",
        "    update_rag_button.click(\n",
        "        app.update_rag_content,\n",
        "        inputs=[],\n",
        "        outputs=[rag_update_status]\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Gradio interface defined!\")\n",
        "print(\"üí° Ready to launch the app\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "70f5a62c",
        "outputId": "1d405a55-879b-4957-c25a-d0270ed3997b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4965fd56b7a57abcff.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://4965fd56b7a57abcff.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# AI-Powered Site Auditor - Section 9: Launch Gradio App\n",
        "\n",
        "# Launch the Gradio interface\n",
        "if __name__ == \"__main__\":\n",
        "    demo.launch(debug=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
